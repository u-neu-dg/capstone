{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "In my capstone project I will revisit the server log data of the platform *Sci-Hub* used by John Bohannon for his analysis of Sci-Hub user behaviour published in Science Magazine 2016 (https://www.sciencemag.org/news/2016/04/whos-downloading-pirated-papers-everyone). Sci-Hub is a shadow library website that provides free access to millions of research papers and books, without regard to copyright, by bypassing publishers' paywalls in various ways. (https://en.wikipedia.org/wiki/Sci-Hub).\n",
    "The dataset Bohannon was provided by the Sci-Hub owner covers logs from September 2015 to February 2016 and has 28 million log entries.\n",
    "\n",
    "Using the *March 2020 Public Data File from Crossref*, provided by Crossref on Academic Torrents https://academictorrents.com/details/0c6c3fbfdc13f0169b561d29354ea8b188eb9d63 which includes metadata of 112 million research articles, and mapping it to the Sci-Hub data, I will provide even more explorative options and insights into the behaviour and preferences of Sci-Hub users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project\n",
    "\n",
    "After cleaning the data from the two sources, Sci-Hub and Crossref using the `pandas`library, I will stage the data from both sources in S3. Using a data warehouse in the form of a star schema relational database in Redshift I am going to make the data accessible for analyses via SQL or other suitable tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Describe and Gather Data, Explore, Clean and Save: Sci-Hub\n",
    "\n",
    "The Sci-Hub data consists of 6 text files, one for each month, which are tab-separated:\n",
    "\n",
    "`sep2015.tab, \n",
    "oct2015.tab, \n",
    "nov2015.tab, \n",
    "dec2015.tab, \n",
    "jan2016.tab, \n",
    "feb2016.tab`\n",
    "\n",
    "There are 6 columns of data available, each row is a reference to a download of an article (referenced via the DOI) by a certain user which has further location details.\n",
    "No column names are provided. I am using `dec2015.tab` in the `scihub_data_raw` folder for an exploration with the `pandas` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = \"scihub_data_raw/dec2015.tab\"\n",
    "\n",
    "df_scihub = pd.read_table(path,names=[\"timestamp\", \"doi\", \"user_id\", \"user_country\", \"user_city\", \"user_location\"],encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual content of each of the columns suggests the following schema with column names `timestamp, doi, user_id, user_country, user_city, user_location`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scihub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`timestamp`, `doi`, and `user_id` seem to be available for almost all rows. `user_country`, `user_country` Ã nd`user_location` are not provided for all downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scihub.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `clean_scihub_data` performs the data cleaning as described in the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_scihub_data(df_scihub):\n",
    "    \"\"\"\n",
    "    cleans the data of the Sci-Hub files\n",
    "    requires a dataframe as input\n",
    "    creates the 'day' column for partitioning\n",
    "    removes rows with empty data in columns timestamp, doi and user_id\n",
    "    \"\"\"\n",
    "    df_scihub[\"day\"] = pd.to_datetime(df_scihub[\"timestamp\"]).dt.day\n",
    "    df_scihub[\"timestamp\"] = pd.to_datetime(df_scihub[\"timestamp\"])\n",
    "    df_scihub = df_scihub[~df_scihub[\"timestamp\"].isnull()]\n",
    "    df_scihub = df_scihub[~df_scihub[\"doi\"].isnull()]\n",
    "    df_scihub = df_scihub[~df_scihub[\"user_id\"].isnull()]\n",
    "    df_scihub[\"day\"] = df_scihub[\"day\"].astype(int)\n",
    "    return df_scihub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean all files in `scihub_data_raw` (samples in the folder: `dec2015.tab` and `jan2016.tab`) and save them in the `scihub_data`folder in parquet format, partitioned by `day`, I am using the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"scihub_data_raw/\"\n",
    "\n",
    "filenames=glob.glob(os.path.join(path, '*.tab'))\n",
    "for filename in filenames:\n",
    "    print(\"loading {}\".format(filename))\n",
    "    \n",
    "    df = pd.read_table(filename,names=[\"timestamp\", \"doi\", \"user_id\", \"user_country\", \"user_city\", \"user_location\"],encoding=\"UTF-8\")\n",
    "    \n",
    "    df = clean_scihub_data(df)\n",
    "    new_filename = filename.replace(\"_raw\",\"\").replace(\".tab\",\".parquet\")\n",
    "    df.to_parquet(new_filename, partition_cols=[\"day\"], engine='pyarrow')\n",
    "    print(\"saved {}\".format(new_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I uploaded these files manually to S3 bucket `s3://scihub-data/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Describe and Gather Data, Explore, Clean and Save: Crossref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crossref data includes metadata for each DOI registered at Crossref. The file format is JSON, there are 37000 single files.\n",
    "I will look at one file, `0.json.gz` to analyse the data structure locally. In the project folder `crossref_data_raw` I saved 33 sample files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"crossref_data_raw/0.json.gz\"\n",
    "with gzip.open(path) as file:\n",
    "    json_object = json.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the JSON object in each file contains a list of `items`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(json_object)[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and there are 3000 items in  each of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(json_object[\"items\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one item is a nested JSON with a large amount of metadata, but for our purpose we will not need all of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object[\"items\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to extract the columns `\"doi\", \"type\", \"title\", \"published-print\", \"prefix\", \"publisher\", \"subject\"` including some data cleaning as described in the docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_crossref_data(json_object):\n",
    "    \"\"\"\n",
    "    extracts the items of the Crossref file\n",
    "    flattens the JSON\n",
    "    extracts columns:\n",
    "    \"doi\", \"type\", \"title\", \"published-print\", \"prefix\", \"publisher\", \"subject\"\n",
    "    remove all rows where \"doi\" is null\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(json_object[\"items\"])\n",
    "    df = pd.json_normalize(json_object[\"items\"])\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df[\"title\"]=df[\"title\"].str[0]\n",
    "    df[\"published-print\"]=df[\"published-print.date-parts\"].astype(str).str.extract(r'(\\d{4})')\n",
    "    if \"subject\" in df.columns:\n",
    "        df[\"subject\"]=df[\"subject\"].str[0]\n",
    "    else:\n",
    "        df[\"subject\"]=np.nan\n",
    "    df=df[~df[\"doi\"].isnull()]\n",
    "\n",
    "    return df[[\"doi\", \"type\", \"title\", \"published-print\", \"prefix\", \"publisher\", \"subject\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call function\n",
    "df_crossref=clean_crossref_data(json_object)\n",
    "df_crossref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking for Null values, there are some titles and published-print values missing, and unfortunately most of the subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crossref.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to clean all files in `crossref_data_raw` and save them in folder `crossref_data` in zipped JSON format, I am using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'crossref_data_raw/'\n",
    "\n",
    "filenames=glob.glob(os.path.join(path, '*.gz'))\n",
    "for filename in filenames:\n",
    "    print(\"loading {}\".format(filename))\n",
    "    \n",
    "    with gzip.open(filename) as file:\n",
    "        json_object = json.load(file)\n",
    "        file.close()\n",
    "        \n",
    "    df_crossref=clean_crossref_data(json_object)\n",
    "    \n",
    "    new_filename = filename.replace(\"_raw\",\"\").replace(\".json.gz\",\"_processed.json\")\n",
    "    \n",
    "    df_crossref.to_json(new_filename, orient='records', lines=True)\n",
    "    #df_crossref.to_parquet(new_filename, engine='pyarrow')\n",
    "    \n",
    "    print(\"saved {}\".format(new_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I uploaded these files manually to S3 bucket `s3://crossref-sample/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "First of all, there are two staging tables based on the two sources, Sci-Hub (table name `scihub_data`) and Crossref (table name `crossref_data`)\n",
    "\n",
    "I decided using a star schema with `downloads` as fact table with supporting dimension tables `time` and `users` which have the SciHub data as source.\n",
    "\n",
    "`downloads.timestamp` links to `time.timestamp`\n",
    "`downloads.user_id` links to `users.user_id`\n",
    "\n",
    "\n",
    "Table `articles` and `publishers` are dimension tables which have Crossref as data source.\n",
    "\n",
    "`downloads.doi` links to `articles.doi`\n",
    "\n",
    "As a normalization step I separated the publishers data from the articles to reduce redundancy.\n",
    "\n",
    "`articles.prefix` links to `publishers.prefix`\n",
    "\n",
    "\n",
    "Using this schema, you'll be able do explore all aspects of the downloads in a convenient way.\n",
    "\n",
    "![schema](Capstone_project_schema.png \"Schema\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "To pipeline the data into the the data model, the following steps are performed:\n",
    "    \n",
    "Read the data from the two S3 Buckets and stage it in the Redshift tables `scihub_data` and `crossref_data`\n",
    "\n",
    "Create the tables and insert the data into Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Let's assume we have a Redshift cluster with a database created with the neccessary authorizations to connect to S3 stored in `redshift.cfg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('redshift.cfg'))\n",
    "\n",
    "REGION_NAME            = config.get(\"AWS\",\"REGION_NAME\")\n",
    "\n",
    "#Redshift credentials\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "DWH_ENDPOINT           = config.get(\"DWH\",\"DWH_ENDPOINT\")\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\",\"DWH_IAM_ROLE_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### connect to Redshift database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT, DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### staging table scihub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS scihub_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE IF NOT EXISTS scihub_data (\n",
    "    timestamp TIMESTAMP NOT NULL, \n",
    "    doi varchar(256), \n",
    "    user_id varchar(256),\n",
    "    user_country varchar(256),\n",
    "    user_city varchar(256),\n",
    "    user_location varchar(256),\n",
    "    day INT8); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scihub_data_copy=\"\"\"\n",
    "COPY scihub_data\n",
    "FROM 's3://scihub-data/'\n",
    "CREDENTIALS 'aws_iam_role={}'\n",
    "FORMAT AS PARQUET;\n",
    "\"\"\".format(DWH_IAM_ROLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql $scihub_data_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### staging table crossref_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DROP TABLE IF EXISTS crossref_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS crossref_data (\n",
    "    doi varchar(256), \n",
    "    \"type\" varchar(256), \n",
    "    title varchar(512),\n",
    "    \"published-print\" INT,\n",
    "    prefix varchar(256),\n",
    "    publisher varchar(256),\n",
    "    subject varchar(256)); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossref_data_copy=\"\"\"\n",
    "COPY crossref_data\n",
    "FROM 's3://crossref-sample/'\n",
    "CREDENTIALS 'aws_iam_role={}'\n",
    "REGION '{}'\n",
    "JSON AS 'auto';\n",
    "\"\"\".format(DWH_IAM_ROLE_NAME, REGION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql $crossref_data_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### downloads table (fact table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DROP TABLE IF EXISTS downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS downloads\n",
    "    (\n",
    "    download_id INT IDENTITY (0,1),\n",
    "    timestamp TIMESTAMP NOT NULL,\n",
    "    doi VARCHAR(256) NOT NULL,\n",
    "    user_id VARCHAR(256) NOT NULL,\n",
    "    PRIMARY KEY(download_id)\n",
    "    );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "INSERT INTO downloads \n",
    "    (\"timestamp\",\n",
    "    \"doi\",\n",
    "    \"user_id\")\n",
    "SELECT \"timestamp\",\n",
    "       \"doi\",\n",
    "       \"user_id\"\n",
    "FROM scihub_data\n",
    "WHERE timestamp IS NOT NULL\n",
    "AND doi IS NOT NULL\n",
    "AND user_id IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### users table (dimension table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DROP TABLE IF EXISTS users;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS users\n",
    "    (\n",
    "    user_id VARCHAR(256) NOT NULL,\n",
    "    user_country VARCHAR(256),\n",
    "    user_city VARCHAR(256),\n",
    "    user_location VARCHAR(256)\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "INSERT INTO users \n",
    "    (\"user_id\",\n",
    "    \"user_country\",\n",
    "    \"user_city\",\n",
    "    \"user_location\")\n",
    "SELECT DISTINCT \"user_id\",\n",
    "       \"user_country\",\n",
    "       \"user_city\",\n",
    "       \"user_location\"\n",
    "FROM scihub_data\n",
    "WHERE user_id IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### time table (dimension table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS time\n",
    "    (\n",
    "    timestamp TIMESTAMP,\n",
    "    year INT,\n",
    "    month INT,\n",
    "    week_of_year INT,\n",
    "    weekday INT,\n",
    "    day INT,\n",
    "    hour INT,\n",
    "    PRIMARY KEY(timestamp)\n",
    "    );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "INSERT INTO time \n",
    "    (\"timestamp\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"week_of_year\",\n",
    "    \"weekday\",\n",
    "    \"day\",\n",
    "    \"hour\")\n",
    "SELECT DISTINCT timestamp           AS timestamp,\n",
    "    EXTRACT(year FROM timestamp)    AS year,\n",
    "    EXTRACT(month FROM timestamp)   AS month,\n",
    "    EXTRACT(week FROM timestamp)    AS week_of_year,\n",
    "    EXTRACT(weekday FROM timestamp) AS weekday,\n",
    "    EXTRACT(day FROM timestamp)     AS day,\n",
    "    EXTRACT(hour FROM timestamp)    AS hour\n",
    "FROM scihub_data\n",
    "WHERE timestamp IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### articles table (dimension table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DROP TABLE IF EXISTS articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS articles\n",
    "    (\n",
    "    doi VARCHAR(256) NOT NULL,\n",
    "    title VARCHAR(512),\n",
    "    \"published-print\" INT,\n",
    "    prefix VARCHAR(256),\n",
    "    PRIMARY KEY(doi)\n",
    "    );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "INSERT INTO articles \n",
    "    (\"doi\",\n",
    "    \"title\",\n",
    "    \"published-print\",\n",
    "    \"prefix\")\n",
    "SELECT DISTINCT \"doi\",\n",
    "       \"title\",\n",
    "       \"published-print\",\n",
    "       \"prefix\"\n",
    "FROM crossref_data\n",
    "WHERE doi IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### publishers table (dimension table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DROP TABLE IF EXISTS publishers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS publishers\n",
    "    (\n",
    "    prefix VARCHAR(256) NOT NULL,\n",
    "    publisher VARCHAR(256),\n",
    "    PRIMARY KEY(prefix)\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "INSERT INTO publishers \n",
    "    (\"prefix\",\n",
    "    \"publisher\")\n",
    "SELECT DISTINCT \"prefix\",\n",
    "       \"publisher\"\n",
    "FROM crossref_data\n",
    "WHERE prefix IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data quality check 1: count number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM scihub_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM crossref_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM downloads;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM users;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM time;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM articles;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM publishers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### quality check 2: check if primary key is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM scihub_data WHERE timestamp IS NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM crossref_data WHERE doi IS NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM downloads WHERE download_id IS NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM users WHERE user_id IS NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM time WHERE timestamp IS NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM articles WHERE doi IS NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM publishers WHERE prefix IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary=pd.read_table(\"data_dictionary.txt\")\n",
    "data_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Clearly state the rationale for the choice of tools and technologies for the project.**\n",
    "\n",
    "Since they datasetes are very large, I choose to store the datasets for the staging tables in S3 because it can be picked up easily by Redshift. Redshift then allows all kinds of querying the data easily using SQL or an UI. Once the data is loaded you can perform analytical queries like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the number of articles which are found in both dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "SELECT count(*)  \n",
    "FROM downloads d\n",
    "JOIN articles a\n",
    "ON d.doi = a.doi;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the most popular titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "SELECT count(a.title) AS title_count,\n",
    "       a.title\n",
    "FROM downloads d\n",
    "JOIN articles a\n",
    "ON d.doi = a.doi\n",
    "GROUP BY a.title\n",
    "ORDER BY title_count DESC\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cities where most users are coming from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT count (user_city) AS user_city_count,\n",
    "user_city\n",
    "FROM users\n",
    "GROUP BY user_city\n",
    "ORDER BY user_city_count DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Propose how often the data should be updated and why.**\n",
    "\n",
    "Supposedly we would receive regular and up to date chunks of the server logs from Sci-Hub, e.g. monthly, we would also have to consequently update the Crossref dataset with the most recent articles to have the chance to match as many DOI as possible. There is an Crossref API that could be called, or we could use the latest Public Data File from Crossref available at `Academic Torrents` (currently, the most recent dataset was published in Jan 2021 https://academictorrents.com/details/e4287cb7619999709f6e9db5c359dda17e93d515)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Write a description of how you would approach the problem differently under the following scenarios:\n",
    "* **The data was increased by 100x.**\n",
    "    I would increase the size of the Redshift cluster to handle this data\n",
    "    \n",
    "* **The data populates a dashboard that must be updated on a daily basis by 7am every day.**\n",
    "    I would create a pipeline with Airflow that also includes data loading and the cleaning steps I performed, as well as any tasks to update the Redshift tables.\n",
    "    \n",
    "* **The database needed to be accessed by 100+ people.**\n",
    "Since I am using Redshift, I would just need to increase the size of the Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
